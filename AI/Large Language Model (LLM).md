<p align='right'><a align="right" href="https://github.com/iamkirankumaryadav/Library/blob/main/Interview.md">Back to Questions</a></p>

# **Large Language Model (LLM)**

A type of AI program that excels at understanding and generating human language. 

1. **Function:**
* LLMs are trained on massive amounts of text data, which allows them to grasp the language and perform various tasks related to NLP.
* These tasks include things like generating text, translating languages, writing creative content, and answering questions in an informative way.

2. **Learning:**
- They use a specific **neural network** architecture called a **transformer model**.
- This model is like a super-powered learning tool that can analyze massive amounts of text data and identify patterns.
- By recognizing these patterns, the LLM learns the relationships between words and their use in context.

### **Facts:**
- Outside of the enterprise context, it may seem like LLMs have arrived unexpectedly along with new developments in generative AI.
- However, many companies have spent years implementing LLMs at different levels to enhance their NLU and NLP capabilities.
- This has occurred alongside advances in ML, ML models, algorithms, neural networks and the transformer models that provide the architecture for these AI systems.
- LLMs are a class of foundation models, which are trained on enormous amounts of data to provide the foundational capabilities needed to drive multiple use cases and applications and resolve many tasks. The idea of building and training domain-specific models for each of these use cases individually is prohibitive under many criteria (most importantly cost and infrastructure)
- LLMs represent a significant discovery in NLP and AI and are easily accessible to the public through interfaces like `Open AI’s Chat GPT-3` and `GPT-4`, which have garnered the support of Microsoft.
- Other examples include `Meta’s Llama` models and Google’s bidirectional encoder representations from transformers `(BERT/RoBERTa)` and `PaLM` models. 
- They can understand from context, generate readable and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or code generation tasks. 

### **Generative AI:** 
- A subfield of AI focused on creating entirely new content, like text, images, or even code.
- It's distinct from traditional AI tasks like analyzing data because it involves a creative element of "imagination" based on learned patterns.

### **Neural Networks:** 
- Inspired by the human brain, they are complex algorithms that learn from vast amounts of data.
- Generative models are a specific type of neural network architecture.

### **Machine Learning:** 
- Training algorithms on data to learn and improve at a specific task, like generating realistic images of cats.

### **Deep Learning:** 
- This is a powerful subfield of ML using complex neural networks, particularly effective for generative AI tasks.
- Deep learning models can analyze intricate patterns in data to produce sophisticated outputs.

### **Natural Language Processing (NLP):** 
- It's crucial for generative models that work with text, like writing different kinds of creative content or translating languages.

### **Generator:** 
- The specific part of a generative AI model responsible for creating new content.
- It takes input data (like a text prompt) and uses its knowledge to produce a novel output.

### **Transformer:** 
- A specific type of neural network architecture particularly well-suited for generative tasks, especially involving text.
- It excels at capturing long-range dependencies within data, allowing for more nuanced and coherent outputs.

e.g. Chat GPT (Human quality text generation) | DALL-E (Creating surreal and realistic images)
